{
 "metadata": {
  "name": "",
  "signature": "sha256:099988790d5b2ba3e919b7a20a153ea62c4836cfa14e19ed7fa269e34913d92f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Ensemble Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">...in an abundance of counselors there is safety. - Prov. 11:14"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In today's class we are going to be tackling the topic of Ensemble Methods.\n",
      "## Part I: Basic Principles\n",
      "* What is ensembling? \n",
      "* What does ensembling give us that we don't already have? \n",
      "* How do we maximize the benefits of ensembling?\n",
      "\n",
      "## Part II: Creating Ensembles\n",
      "* What is the two step process for creating an ensembling algorithm? \n",
      "* What are some different approaches for following this process? \n",
      "\n",
      "## Part III: Random Forests\n",
      "* Discuss the benefits of the Random Forest Algorithm\n",
      "* Describe the Random Forest Algorithm\n",
      "\n",
      "## Part IV: Exercise\n",
      "* Create a Random Forest\n",
      "* Compare the Random Forest with CART\n",
      "* Calculate the gini scores\n",
      "* Tune across multiple number of tree values\n",
      "\n",
      "## Bonus: Adaboost\n",
      "* Bonus: If we have time, we will also cover the Boosting, which is an alternative tree-based ensembling alternative to the Random Forest."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part I: Basic Principles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we will cover two core questions: \n",
      "* What is ensembling and why should I care?\n",
      "* Why does ensembling work the way that it does?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is an ensemble method? **A method that creates and combines a committee of models.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Question: ** Given a task that requires making a decision, particularly one with an objective right answer, which is more effective, a single person or a committee of people? \n",
      "<img src=\"dilbert-meeting-2.gif\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Why should I care about ensembling?*\n",
      "<br /> John Elder IV of Elder Research and Sauchi Stephen Lee of University of Idaho conducted a [study](http://datamininglab.com/media/pdfs/lee_elder_97.pdf) where they trained 5 different models on 6 publicly available datasets. The results are shown below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"individual_predictors.PNG\">\n",
      "Next, they combined the individual predictors to create committee predictions on the same data set. The results are shown below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"ensemble_predictors.PNG\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Explanation\n",
      "These results were quite counterintuitive to me when I first heard of them. Why is it that ensembling improved the predictive accuracy?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Question: ** Given a task that requires making a decision, particularly one with an objective right answer, which is more effective, a single person or a committee of people?  <br />  <br />This depends on several factors: \n",
      "- Do the people agree on the definition of \"truth\"?\n",
      "- How likely is each person to choose the truth on his/her own? \n",
      "- Do the people come from a diverse background?\n",
      "\n",
      "If we assume answers to the aforementioned questions, we can test our our idea about the improvement that ensemble methods provide through an application of the binomial distribution "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Brief Aside: Binomial Distribution\n",
      "The [binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution) is a discrete probability distribution that provides the probability of getting exactly k outcomes in n, independent trials given a prior probability of success. For example: A student has 50% chance of getting into each graduate schools she applies to and her admission to a given graduate program has no bearing on her admission to any other graduate program. Given that she is applying to 20 graduate programs, what is the probability that she gets into exactly 10? What is the probability that she gets into more than 10?\n",
      "<img src=\"binomial_distribution.PNG\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Armed with this foundation, we can make an inference about the relative accuracy of an ensemble model. In this application, our 'trials' will be different models, and our 'outcome' will be the the model making an incorrect prediction. For the purposes of demonstration, we model the probability of the ensemble making an incorrect prediction as the probability that more than half of the models will make an incorrect prediction.\n",
      "<img src=\"condorcets_jury_theorem.png\">\n",
      "As you can see, the probability of error decreases as more models are added to the voting process. This is also known as \n",
      "[Condorcet's Jury Theorem](http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem), which was developed by a French political scientist back in the 18th century."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, this makes sense, if a group of people, with different strengths and weaknesses come together for a common objective, they are more likely to arrive at the truth, and a single person, because each person will likely have different information and a unique perspective. For ensembling then, the models should be different from one another to truly realize the benefit of ensembling. To put a slightly finer point, the models should be different from each other in terms of their local ability to fit the data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part II: Creating Ensemble Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two basic steps to creating an ensemble model. \n",
      "1. Create the predictive models\n",
      "2. Combine the predictions for each predictive model\n",
      "\n",
      "#### Create the predictive models\n",
      "The key with creating predictive models, is to create models that are different from one another. This is your only hope of creating trees that compliment each other. The best way of creating predictive models will be dependent on the specifics of your situation, but the general principlpe to follow is to create models that are different from each other. With that being said, here are some strategies for creating models that produce different results from each other.\n",
      "* Using different subsets of the input data to train the model. This is also known as bootstrap aggregation, or bagging. \n",
      "* Choosing different combinations of features to train the model.\n",
      "* Changing the tuning parameters used to guide the training process.\n",
      "* Training with different modeling techniques.\n",
      "\n",
      "#### Combine the predictions\n",
      "We we saw earlier, there are several ways to combine models. Some of the most common methods include the following:\n",
      "* Majority vote (applicable for discrete output)\n",
      "* Averaging (applicable for continuous output, or discrete output) \n",
      "* Stacking: When a series of model predictions are stacked, they are used as input data for another algorithm that seeks to correlation each individual prediction with the target outcome. I am told that the logistic regression is a common technique used for model stacking."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part III: The Random Forest Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br /> Invented by Leo Breiman and Adele Cutler in 2001, the Random Forest algorithm is one of the most popular ensembling methods because it automates a lot of the model building process and tends to produce impressive results. The original [paper](http://oz.berkeley.edu/~breiman/randomforest2001.pdf) is surprisingly easy to read, and I will use quotes from the book to guide our discussion.\n",
      "\n",
      "<img style=\"float: right\" src=\"Leo_Breiman.jpg\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Benefits of Random Forests\n",
      "> 1. Its accuracy is as good as Adaboost and sometimes better.*\n",
      "2. It's relatively robust to outliers and noise.\n",
      "3. It's faster than bagging or boosting.\n",
      "4. It gives useful internal estimates of error, strength, correlation\n",
      "and variable importance.\n",
      "5. It's simple and easily parallelized. (p. 7)\n",
      "\n",
      "<br /> *Adaboost is a technique that we haven't covered yet, however, it is another popular ensembling technique. If we have time on Thursday we will attempt to cover this topic as well."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Definition of Random Forests\n",
      "\n",
      "> Each new training set is drawn, with replacement,\n",
      "from the original training set. Then a tree is grown on the new training set using\n",
      "random feature selection. The trees grown are not pruned. (p. 8)\n",
      "\n",
      "<br /> As you can see, the Random Forest is an ensemble method that is built off of creating individual decision trees. Are their any lingering questions from the class on Decision Trees? Let's review the Decision Tree algorithm briefly. \n",
      "\n",
      "\n",
      "<br /> Why are trees not pruned? To increase variance (i.e., model diversity) Variance is good because it means we have different trees. Remember how this was a disadvantage we discussed of CART? Well, now this is a key feature that makes enables the Random Forest to be so powerful. There is a good reason why the tree-based methods are particularly well suited for out-of-the-box ensembling. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Tuning the Random Forest\n",
      "> The generalization error for forests converges to a limit\n",
      "as the number of trees in the forest becomes large. (p. 1)\n",
      "\n",
      "<br />\n",
      "By generalization error, Breiman is referring to the error on the test dataset. We will see what he means when the error converges. This is a rather unique tuning case, since it does not ever overfit the data. To get an intution for this, think about the sample size for your survey, at first, your understanding of the demographic will improve  as you increase your sample size, however, after a while, you are going to see diminishing marginal returns. It is the same way with increasing the number of trees for the Random Forests."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Random Forest, CART Comparison\n",
      "<img src=\"CART_RF_Adaboost_comparison.png\">\n",
      "As you can see, Random Forest consistently outperforms the CART algorithm, and it is on par (sometimes better, sometimes worse) than the Adaboost algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part IV: Exercise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This exercise builds off of the code used in part one of class, on classification and regression trees. First we will replicate the results we achieved in the first class. Then you will split into pairs and implement a Random Forests. \n",
      "1. Create a Random Forest classifier using: `from sklearn.ensemble import RandomForestClassifier`\n",
      "2. Calculate the ROC AUC for the Random Forest. How does it compare with CART? \n",
      "3. Calculate the feature scores for the Random Forest. Which feature is the most important, which is the least important?\n",
      "4. Tune Random Forest across multiple tree values and plot the results. How many trees would you choose?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Import packages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd C:\\Users\\josdavis\\Documents\\Personal\\GitHub\\DAT6\\data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Users\\josdavis\\Documents\\Personal\\GitHub\\DAT6\\data\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import tree\n",
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Prepare data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in the data\n",
      "titanic = pd.read_csv('../data/titanic.csv')\n",
      "\n",
      "# Take a  selection of the variables\n",
      "d = titanic[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']]\n",
      "\n",
      "# Convert all variables to numeric so for scikit learn\n",
      "d['Sex'] = np.where(d.Sex == 'female', 1, 0)\n",
      "\n",
      "# Fill in missing values with the mean value\n",
      "d['Age'] = d['Age'].fillna(d['Age'].mean())\n",
      "\n",
      "# Create a proxy variable representing whether the Spouse was on board\n",
      "d['Spouse'] = ((d.Age > 18) & (d.SibSp >= 1)).astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, split the data into training and test sets\n",
      "train, test = train_test_split(d,test_size=0.3, random_state=1)\n",
      "\n",
      "# Convert them back into dataframes, for convenience\n",
      "train = pd.DataFrame(data=train, columns=d.columns)\n",
      "test = pd.DataFrame(data=test, columns=d.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Create a CART Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create and fit the decision tree\n",
      "ctree = tree.DecisionTreeClassifier(random_state=1)\n",
      "ctree.fit(train.drop('Survived', axis=1), train['Survived'])\n",
      "\n",
      "# Calculate the predicted probabilities\n",
      "probs = ctree.predict_proba(test.drop('Survived', axis=1))[:,1]\n",
      "metrics.roc_auc_score(test['Survived'], probs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "0.73259448707019037"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Create a Random Forest classifier using: `from sklearn.ensemble import RandomForestClassifier`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Calculate the ROC AUC for the Random Forest. How does it compare with CART? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Calculate the feature scores for the Random Forest. Which feature is the most important, which is the least important?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. Tune Random Forest across multiple tree values and plot the results. How many trees would you choose?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forests are commonly tuned along two parameters:\n",
      "* The number of trees (`n_estimators`)\n",
      "* The number of variables to choose from at each split (`max_features`)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bonus: The Adaboost Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a good time to pause and ask: how does one go about learning a new algorithm?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some of the resources I go to first:\n",
      "* [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n",
      "* [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) and the [video companions](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/).\n",
      "* [scikit-learn documentation](http://scikit-learn.org/stable/modules/classes.html)\n",
      "* Papers referenced in scikit-learn documentation      "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How does the adaboost algorithm work? Let's check out the scikit learn [documentation](http://scikit-learn.org/stable/modules/ensemble.html#adaboost) for adaboost"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. \n",
      "\n",
      "> The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. \n",
      "\n",
      "> The data modifications at each so-called boosting iteration consist of applying weights w_1, w_2, ..., w_N to each of the training samples. Initially, those weights are all set to w_i = 1/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How are the weights calculated? Let's check out the Elements of Statistical Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Source: **Elements of Statistical Learning (page 339)**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"adaboost_algorithm.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notes on the terminology used:\n",
      "* Step 2: m represents a single model (e.g., a decision stump)\n",
      "* Step 2(b): I() is the indicator function, which returns 1 when the condition is true, and 0 when the condition is not true\n",
      "* Step 2(c): alpha and the error term are a single value that describe each model. Although it might not be obvious from the equation, when the error increases, alpha decreases, and when error decreases, alpha increases. \n",
      "* Step 2(d): Many Adaboost algorithms decrease the weights of correctly predicted points, however, step (d) shown here does not do this. If this algorithm were to be changed to deprioritize the weights, step (d) would be w_i = w_i * exp(-alpha) for correctly predicted observations and w_i = w_i * exp(alpha) for incorrectly predicted observations."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What does all of this mean? Let's go through a visual example to help us gain some intuition on how the model works."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Source: **[Tutorial](http://www.cs.princeton.edu/~schapire/talks/nips-tutorial.pdf) by Dr. Rob Schapire of Princeton**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's say we have a training set with 10 data points and two continuous features. There are 5 data points with a '+' class and five with a '-' class. The horizontal and vertical dimensions represent the two dimensions of the feature space.\n",
      "<img src=\"adaboost1.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the first model\n",
      "We then learn a decision stump (i.e., a single split) on the data which defines the decision surface (background colors). Next, we increase the weight of the misclassified observations and decrease the weight of the correctly classified observations.\n",
      "<img src=\"adaboost2.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the second model\n",
      "Now we learn a new decision stump based off of the weighted data. Notice that this produces a different decision surface than the previous model. Then, as before, we update the weights based off of which observations were and were not classified correctly.\n",
      "<img src=\"adaboost3.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the third model\n",
      "Now we learn our third (and final) decision stump based off of the reweighted data. Notice that this produces a different decision surface than the previous two models. Then, as before, we update the weights based off of which observations were and were not classified correctly.\n",
      "<img src=\"adaboost4.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Combining the individual models\n",
      "Now, we have three decision surfaces, each of which has an error and an alpha value. We can combine these decision surfaces by adding using the alpha values that were calculated for each model. \n",
      "<img src=\"adaboost5.png\">\n",
      "The end result gives us a more sophisticated and more accurate decision boundary than we had from any individual decision stump. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's create an AdaBoost classifier in scikit-learn. If you have followed the code for the Random Forest, then implementing the adaboost algorithm in python is trivial. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Train an AdaBoost Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import AdaBoostClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bt = AdaBoostClassifier(random_state=1)\n",
      "bt.fit(train.drop('Survived', axis=1), train['Survived'])\n",
      "probs = bt.predict_proba(test.drop('Survived', axis=1))[:,1]\n",
      "metrics.roc_auc_score(test['Survived'], probs) # 81.55%"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "0.81554418868996859"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Calculate Feature Importance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imp['AdaBoost'] = bt.feature_importances_\n",
      "imp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>variable</th>\n",
        "      <th>cart</th>\n",
        "      <th>RF</th>\n",
        "      <th>AdaBoost</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Pclass</td>\n",
        "      <td> 0.140505</td>\n",
        "      <td> 0.140797</td>\n",
        "      <td> 0.14</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>    Sex</td>\n",
        "      <td> 0.389290</td>\n",
        "      <td> 0.328743</td>\n",
        "      <td> 0.06</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>    Age</td>\n",
        "      <td> 0.340557</td>\n",
        "      <td> 0.377435</td>\n",
        "      <td> 0.62</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  SibSp</td>\n",
        "      <td> 0.082354</td>\n",
        "      <td> 0.060387</td>\n",
        "      <td> 0.10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  Parch</td>\n",
        "      <td> 0.034359</td>\n",
        "      <td> 0.071277</td>\n",
        "      <td> 0.08</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> Spouse</td>\n",
        "      <td> 0.012934</td>\n",
        "      <td> 0.021360</td>\n",
        "      <td> 0.00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "  variable      cart        RF  AdaBoost\n",
        "0   Pclass  0.140505  0.140797      0.14\n",
        "1      Sex  0.389290  0.328743      0.06\n",
        "2      Age  0.340557  0.377435      0.62\n",
        "3    SibSp  0.082354  0.060387      0.10\n",
        "4    Parch  0.034359  0.071277      0.08\n",
        "5   Spouse  0.012934  0.021360      0.00"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Tune along the number of trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bt = AdaBoostClassifier(random_state=1)\n",
      "n_estimators = list(xrange(1, 30, 2)) + list(xrange(30, 101, 10))\n",
      "param_grid = dict(n_estimators=n_estimators)\n",
      "grid = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc')\n",
      "grid.fit(d.drop('Survived', axis=1), d['Survived'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "GridSearchCV(cv=5,\n",
        "       estimator=RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=1, verbose=0),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'n_estimators': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30, 40, 50, 60, 70, 80, 90, 100]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
        "       scoring='roc_auc', verbose=0)"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Plot the tuning results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_mean_scores = [result[1] for result in grid.grid_scores_]\n",
      "plt.xlim([0,100])\n",
      "plt.scatter(n_estimators, grid_mean_scores, s=40)\n",
      "plt.grid(True)\n",
      "plt.title('Tuning AdaBoost Along the Number of Trees')\n",
      "plt.ylabel('AUC for 5-fold CV')\n",
      "plt.xlabel('Number of Trees')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "<matplotlib.text.Text at 0x17067908>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEZCAYAAABrUHmEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzEnDzMxq5qRhZmY1c9IwM7Oa/X9bckCtvrMsawAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1702e3c8>"
       ]
      }
     ],
     "prompt_number": 53
    }
   ],
   "metadata": {}
  }
 ]
}